\documentclass[a4paper, oneside]{discothesis}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{CJKutf8}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT METADATA

\thesistype{Towards more data-driven, and less biased decisions in startup investing with AI workflows} % Master's Thesis, Bachelor's Thesis, Semester Thesis, Group Project
\title{Moon AI: automating VC}

\author{Angelo Giacco}
\email{agiacco@ethz.ch}

\institute{ETH AI Centre \\[2pt]
ETH Zürich}

\logo{\includegraphics[width=0.2\columnwidth]{figures/eth_ai_center}}

\supervisors{Isabelle Siegrist\\[2pt] Prof. Dr. Elliott Ash}

% Optionally, keywords and categories of the work can be shown (on the Abstract page)
%\keywords{Keywords go here.}
%\categories{ACM categories go here.}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\frontmatter % do not remove this line
\maketitle

\cleardoublepage

\begin{acknowledgements}
	I would like to express my sincere gratitude to several individuals who have been instrumental in the completion of this thesis. First and foremost, I extend my heartfelt thanks to my advisor, Isabelle Siegrist, for her invaluable guidance, and to Professor Elliott Ash for his mentorship. It was only by pure chance that we collaborated, but I am very grateful that we did.

A big thanks to everyone at Beugi that made my year in Switzerland so wonderful and full of hike/ski days.

I owe as always a special debt of gratitude to my family for their love and support.

    \begin{CJK*}{UTF8}{gbsn}
最后，我要特别感谢我的伴侣萧庭恩。她始终如一的支持和对我的信任是我前进的动力。\clearpage\end{CJK*}
\end{acknowledgements}



\begin{abstract}
    The venture capital (VC) industry has in recent years experienced a significant shift towards a solo General Partner (GP) model, where a single partner, often a successful former founder or early hire at a unicorn, raises a fund to invest in upcoming entrepreneurs. Solo-GPs are more agile but lack the platform services and in depth analysis available to VC firms with large teams. Simultaneously, Large Langauge Models (LLMs) have emerged as powerful multi-task learners. This has led to the emergence of AI-copilots powered by LLMs for multi-task reasoning emerging in various verticals. 
    
    This thesis explores the creation of a new product: MoonAI. MoonAI is a venture investing copilot designed to empower solo GPs and traditional VC firms alike, by providing a copilot capable of emulating a team of analysts. By employing large language models for reasoning, structured output to guide agents, and matryoshka embeddings to semantically search the internet, the system extracts relevant signals for startup investing from vast amounts of unstructured data publicly available on the internet. The end result is a tool that helps assist in sourcing startups and generating investment memos.
    
    MoonAI can be customized to align with various investment hypotheses, supporting solo GPs in their sourcing and due diligence processes and removing the need for duplicate platform teams at each VC firm. MoonAI aims to evolve into a comprehensive data science and investing platform.
    
    By creating this adaptable infrastructure, MoonAI seeks to democratise access to high-quality investment platforms, encourage data-driven investing, and thereby democratise access to capital. The project's core thesis posits that successful VC firms and especially solo-GPs will increasingly rely on automated research analysts to support upcoming entrepreneurs, with MoonAI positioning itself at the forefront of this transformation.
    
    This thesis provides an overview of the advances in Natural Language Processing that make MoonAI possible, an overview of the VC industry, and details how the product was built. 
\end{abstract}


\begin{zusammenfassung}
Die Venture-Capital-Branche (VC) hat in den letzten Jahren eine bedeutende Verlagerung hin zu einem Modell mit einem einzelnen General Partner (GP) erlebt, bei dem erfolgreiche ehemalige Gründer aufstrebende Unternehmer unterstützen.
Gleichzeitig haben sich Large Language Models (LLMs) als leistungsfähige Multi-Task-Lerner etabliert. Dies hat dazu geführt, dass in verschiedenen Bereichen KI-Copiloten entstanden sind, die sich auf LLMs für Multi-Task-Reasoning stützen.
CopyDiese Arbeit untersucht die Entwicklung eines neuen Produkts, MoonAI, eines innovativen Venture-Investing-Copiloten, der sowohl einzelne GPs als auch traditionelle VC-Firmen unterstützen soll, indem er als Copilot fungiert, der ein Team von Analysten emulieren kann.
Durch den Einsatz von Large Language Models für Reasoning, strukturierte Ausgaben zur Steuerung von Agenten und Matryoshka-Embedding für semantische Internetsuchen extrahiert das System relevante Signale für Startup-Investitionen aus großen Mengen unstrukturierter Internetdaten.

Derzeit als Memo-Generierungstool mit einem einfachen CRM-Layout verpackt, strebt MoonAI an, sich zu einer umfassenden Plattform für Data Science und Investitionen zu entwickeln. Das System kann an verschiedene Investitionshypothesen angepasst werden, um einzelne GPs bei ihren Sourcing- und Due-Diligence-Prozessen zu unterstützen und die Notwendigkeit von doppelten Plattform-Teams in jeder VC-Firma zu eliminieren.

Durch die Schaffung dieser anpassungsfähigen Infrastruktur zielt MoonAI darauf ab, den Zugang zu hochwertigen Investitionsplattformen zu demokratisieren, datengetriebenes Investieren zu fördern und dadurch den Zugang zu Kapital zu demokratisieren. Die Kernthese des Projekts postuliert, dass erfolgreiche VC-Firmen und insbesondere einzelne GPs zunehmend auf automatisierte Research-Analysten zurückgreifen werden, um aufstrebende Unternehmer zu unterstützen, wobei sich MoonAI an der Spitze dieser Transformation positioniert.

Diese Arbeit bietet einen Überblick über die Fortschritte in der Natürlichen Sprachverarbeitung, die MoonAI ermöglichen, einen Überblick über die VC-Branche und detaillierte Informationen darüber, wie das Produkt entwickelt wurde.
\end{zusammenfassung}

\tableofcontents

\mainmatter % do not remove this line

% Start writing here
\chapter{Introduction}

The venture capital (VC) industry stands at the forefront of innovation, playing a crucial role
in identifying and nurturing startups that have the potential to disrupt industries and drive
economic growth. Indeed, Acs et al. ~\cite{acs} find that startups are key catalysts for economic growth, 
not only creating new markets but also stimulate competition and innovation in existing ones, bringing economic dynamism across multiple sectors. 

The VC industry invests in startups with significant potential to disrupt industries and drive economic growth. 
VC investments returns follow a power law distribution, with a long tail of successful investments. 
Indeed, a study of 21,000 financings from 2004-2013 by Correlation Ventures found that 65\% of financings fail to return 1x capital ~\cite{levine2014venture}, 
thus the remaining 35\% must return significantly more to generate a net-return that is acceptable. 
VCs rely on a small number of portfolio investments to achieve outstanding paybacks: enough to cover for losses and
still produce substantial profits. To generate these substantial outcomes, startups rely on the external funding from VCs
to achieve rapid growth, often depicted by a "hockey stick" growth curve ~\cite{marmer}.

The power law nature of startups means that the competition to get on the best deals is incredibly intense, and those 
deals are also the only ones that matter. The top 2\% of VC funds capture 95\% of industry
returns ~\cite{bai}. Particularly at the earliest stage of the investment lifecycle (pre- Series A), there is a scarcity of
reliable information ~\cite{dellermann}. As a result, VCs often rely heavily on human judgment which is prone to bias. 
This may yield sub-optimal decisions ~\cite{cummingdai} and could explain why women-led startups only received 2.1\% of all venture capital invested in 2023 ~\cite{pitchbook2024vc}.
Traditionally, VC firms have relied heavily on human intuition, personal networks, and experience
to identify promising investment opportunities. While this approach has yielded success, it is
not without limitations. Human decision-making is susceptible to biases, inconsistencies, and the
constraints of processing vast amounts of information.

VCs have increasingly adopted data-driven investing practices to combat these biases. 
Traditional gradient boosting approaches have been used successfully, and in recent years the growth of data volume has ushered deep learning (DL) into the industy ~\cite{eqt}.
For example, graph neural networks have been used to predict a startup's success based on its links to a wider startup ecosystem ~\cite{korea}. 
These models can integrate diverse data sources, including structured information about startups, founders, and investors ~\cite{corea}, as well as unstructured data from social networks and web sources. 
Preliminary findings suggest that data-driven VCs may be more likely to back founders from underrepresented backgrounds, potentially addressing systemic biases in the industry ~\cite{futureVC}.

In recent years, the venture capital landscape has witnessed a significant shift with the emergence of solo general partners (GPs) and difficult fundraising conditions for non-elite VC firms. 
Unlike traditional VC firms that often require consensus among multiple partners, solo GPs can make investment decisions quickly and independently, which is particularly valuable in competitive deal environments where rapid decision-making can be crucial.
In contrast, the formation of new traditional VC firms has slowed in recent years. This can be attributed to several factors, including the trend towards larger fund sizes, which makes it challenging for new firms to compete and gain traction with the returns from the VC industry becoming concentrated among well-established firms that enjoy significant advantages in deal flow and fundraising. 
The difficulty of smaller VCs to raise funds means they will have less resources for research, contributing to their worse performance.

In addition, solo GPs have extremely limited resources which may amplify the issues of bias and incomplete information in the investment process, forcing them to  focus on a narrower pool of potential investments, potentially overlooking promising opportunities outside their immediate network (Huang et al., 2020). 
Furthermore, without the resources to explore a wide range of sectors or geographies, solo GPs might gravitate towards familiar industries or founder profiles, potentially limiting the diversity of their investment portfolios (Gompers et al., 2021) and limiting opportunities for under represented founders.
In addition, the agility of solo GPs will intensify the competition and rush to produce term sheets for 'hot' startups. 

In response to these challenges, a project at the ETH AI Center of ETH Zurich aims to introduce automation to VC firms and facilitate the adoption of data-driven investing practices. 
The project focuses on developing an AI-powered tool for generating investment memoranda. This approach aims to streamline the decision-making process for investment analysts, significantly reducing the time spent on screening potential investments. 
Utilizing multimodal data and state-of-the-art language models, the system aggregates diverse data sources, including pitch decks, financial data, and web information, to provide a comprehensive analysis of investment opportunities.
Furthermore, this project introduces tools for data-driven investing that aim to reduce bias in the investment process by incorporating objective insights, broadening investment scope, identifying overlooked startups, informing decisions with diverse data points, and standardizing evaluation processes. Developing the MoonAI product is the first phase of this project; subsequent research will analyse its ability to reduced bias decision making. 

The potential impact of such a tool is multifaceted. First, it addresses the resource disparity
between solo GPs/smaller VC firms and larger VC firms, potentially democratizing access to high-quality investment
analysis. Second, it introduces a data-driven approach to complement human intuition in the
investment process, potentially reducing biases and improving decision-making accuracy
~\cite{dellermann}. Finally, by streamlining the sourcing and evaluation processes, MoonAI
could allow VCs to consider a broader range of startups by reducing the cognitive load of analysing a startup, potentially leading to a more diverse
and inclusive investment landscape (Brush et al., 2019).

While the VC industry has been facing challenges, Large Language Models (LLMs) have emerged as powerful tools capable of multi-task reasoning and processing vast amounts of
unstructured data (Brown et al., 2020) and transformed the field of Natural Language Processing (NLP). These advancements have led to the development of
AI-powered copilots in various industries, augmenting human capabilities and decision-making processes.
 Large Language Models (LLMs) are particularly well-suited to be a venture capital copilot and decision-making support for several key reasons. 
Firstly, LLMs excel at pattern matching, a crucial skill in the venture capital industry. They can analyze vast amounts of unstructured data from various sources, 
identifying trends, similarities, and potential success indicators that might not be immediately apparent to human analysts.
They can rapidly process and synthesize information from a wide range of publicly available sources, including news articles, social media, academic publications, 
and industry reports. This ability enables a comprehensive understanding of a startup's market position, competitive landscape, and potential growth trajectory 
without relying solely on information provided by the startup itself.
Furthermore, LLMs offer simple yet powerful workflow automation capabilities. They can streamline many time-consuming aspects of the venture capital process, 
such as initial startup screening, report generation, and even preliminary financial analysis. By automating these routine tasks, LLMs free up valuable time 
for venture capitalists to focus on higher-level strategic decisions and relationship building with promising founders. This efficiency gain is particularly 
beneficial for solo GPs or smaller VC firms with limited resources, allowing them to compete more effectively with larger, more established firms.
The combination of these capabilities makes LLMs an ideal tool for augmenting human decision-making in venture capital. They can provide data-driven insights,
 reduce information asymmetry, and increase the overall efficiency of the investment process. As the venture capital landscape continues to evolve and 
 become more competitive, the integration of LLMs into investment workflows has the potential to significantly enhance the accuracy, speed, and scope of investment decisions, ultimately leading to better outcomes for both investors and entrepreneurs.

The emergence of Large Language Models (LLMs) represents a significant milestone in the fields of artificial intelligence and natural language processing. The development of these models can be traced through several key stages, each marking a crucial advancement in the technology.
The foundations of natural language processing can be traced back to the 1950s, with Alan Turing's seminal work on the "Turing Test" ~\cite{turing1950computing}. Over the subsequent decades, rule-based systems and statistical methods formed the basis of early NLP research, laying the groundwork for future advancements.
The early 2010s saw a resurgence of interest in neural networks, thanks to the ability to train on GPUs, leading to breakthroughs such as word2vec ~\cite{mikolov2013efficient}. This innovation allowed words to be represented as dense vectors, capturing semantic relationships in a way that significantly enhanced the capabilities of NLP systems.
Between 2014 and 2017, the introduction of sequence-to-sequence models ~\cite{sutskever2014sequence} and the attention mechanism ~\cite{bahdanau2014neural} marked another leap forward. These advancements significantly improved machine translation and other NLP tasks.
A pivotal moment came in 2017 with the publication of "Attention is All You Need" ~\cite{vaswani2017attention}, which introduced the Transformer architecture. This innovation became the foundation for modern LLMs, as the transformer architecture allowed for highly parallel 
The years 2018 and 2019 saw the emergence of the pre-training and fine-tuning paradigm. Models such as BERT ~\cite{devlin2018bert} and GPT ~\cite{radford2018improving} demonstrated the power of pre-training on large corpora followed by task-specific fine-tuning, setting new benchmarks in NLP performance.
From 2020 onwards, the focus shifted to scaling up these models. GPT-3 ~\cite{brown2020language} demonstrated that increasing model size and training data could lead to emergent abilities, a trend that has continued with subsequent models such as PaLM, Chinchilla, and GPT-4.
Most recently, from 2022 onwards, there has been a growing emphasis on instruction tuning and alignment. Models like InstructGPT and ChatGPT have highlighted the importance of aligning LLMs with human intent, often through techniques such as reinforcement learning from human feedback (RLHF) ~\cite{ouyang2022training}.
The latest developments in the field include the exploration of agent simulacra and agentic workflows, pushing the boundaries of what LLMs can achieve in terms of task completion and decision-making processes.


This thesis explores the intersection of these trends through the development of MoonAI, an
AI-powered venture investing copilot designed to empower both solo GPs and traditional VC firms.
By leveraging state-of-the-art LLMs, structured output for guided reasoning, advanced
semantic search capabilities, and agentic workflows, MoonAI aims to speed up the startup evaluation process and remove repetitive tasks from a VC. The
system is designed to extract relevant signals for startup investing from the vast pool of
publicly available unstructured data on the internet, assisting in sourcing startups and
generating comprehensive investment memos. By addressing these questions and developing a practical tool for the VC industry, this thesis
aims to contribute to the ongoing dialogue about the role of AI in financial decision-making and
its potential to reshape the startup ecosystem. The following chapters will delve into the
technical foundations of Large Language Models, a review of the Venture Capital industry, and evaluate the MoonAI platform. 

\chapter{Natural Language Processing}

In this section, the recent in advances in Natural Language Processing will be discussed. First, we will consider the basics of machine learning, 
consider the standard, non-deep learning approaches to NLP used in the past and discuss the advances that have been made in recent years that have enabled large language models to become state of the art at virtually all tasks in the field of NLP.

Many of these advances have been possible due to the parallelizability of training thanks to the Transformer architecture which make use of advances in GPUs. How GPUs are able to parallelize the training of large language models is beyond the scope of this overview, but it is important to note that this is a significant advance that has revolutionized the field of NLP. It can be summarised as follows: GPUs have significantly accelerated the training of transformer-based models by leveraging their ability to perform massive parallel computations, which make them ideal for matrix multiplications, the operation underpinning the self-attention mechanism in transformers and the feed-forward layers. 
GPUs excel at these tasks due to their architecture, which is designed to handle many simple calculations simultaneously.
This GPU-driven acceleration has been a key factor in enabling the training of increasingly large and complex language models, pushing the boundaries of what's possible in NLP.

Before we consider the essentials of machine learning that enable the training of large language models, let us first specify the objective of a large language model more clearly. 
This will also motivate the use of the Transformer architecture, and clarify why other methods, from n-gram based approaches to other flavours of DL like recurrent neural networks, are not suited for the task.

\begin{theorem}[Language Modelling Objective] \label{thm:first theorem}
    Given an alphabet $\Sigma$ and a distinguished end-of-sequence symbol $\texttt{eos} \notin \Sigma$, a
    language model is a collection of conditional probability distributions $p(y \mid w)$ for $y \in \Sigma \cup \{\texttt{eos}\}$ and $\w \in \Sigma^*$,
    the Kleene closure of $\Sigma$.
\end{theorem}

We therefore can obtain the probability of a string according to a particular language model by autoregressively factorising:

\begin{equation}
    P(w) = P(w_1, \ldots, w_n) =  P(\texttt{EOS} \mid w) \cdot \prod_{i=1}^n P(w_i \mid w_1, \ldots, w_{i-1}) 
\end{equation}

With the objective of crafting a model able to learn this function p in mind, let us begin our exploration of machine learning. 

\section{Modelling Data}

This thesis assumes some familiarity with the basics of machine learning. Nevertheless, a brief summary is provided here to establish a foundation for the subsequent discussions.

In classical computer programming, humans define a set of explicit rules and provide input data. The computer then processes this data according to the predefined rules to produce an output. 
This approach works well for problems with clear, unchanging rules and predictable inputs ~\cite{russell2010artificial}.

Machine learning, on the other hand, inverts this paradigm. It can be broadly categorized into two main types: supervised and unsupervised learning ~\cite{murphy2012machine}.In supervised learning, humans provide a large dataset of input-output pairs (often referred to as training data). 
The machine learning algorithm then attempts to learn the underlying patterns or rules that map the inputs to the outputs ~\cite{hastie2009elements}. 
This learned model can then be used to make predictions or decisions on new, unseen data.
Supervised learning is particularly useful for tasks such as classification and regression ~\cite{bishop2006pattern}.
In unsupervised learning, the algorithm is given input data without corresponding output labels. The goal is to discover hidden patterns or structures within the data ~\cite{ghahramani2004unsupervised}. Common unsupervised learning tasks include clustering, dimensionality reduction, and anomaly detection ~\cite{hinton2006reducing}.
The key advantage of machine learning is its ability to handle complex, non-linear relationships in data and to adapt to new patterns without being explicitly reprogrammed ~\cite{goodfellow2016deep}. 
This makes it particularly useful for tasks like natural language processing, where the rules governing language use are clearly too complex and nuanced to be explicitly coded ~\cite{jurafsky2009speech}.

\subsection{Losses}
To begin our search for a model that best describes the underlying patterns in our data, it is necessary to find a way of describing how accurate our model's predictions are.
This is done by defining a loss function that measures the difference between the model's predictions and the ground-truth values ~\cite{goodfellow2016deep}.

In the context of natural language processing where we have the option of predicting multiple labels for a single input, an evident choice for the loss function is cross-entropy, which 
has its basis in information theory as a measure of the difference between two probability distributions ~\cite{shannon1948mathematical}.

The cross-entropy loss, denoted as $H(p,q)$, for a classification task with $C$ classes is defined as:

\begin{equation}
    H(p,q) = -\sum_{i=1}^C p(i) \log q(i)
\end{equation}

where $p(i)$ is the true probability distribution (usually a one-hot encoded vector for the true class) and $q(i)$ is the predicted probability distribution from the model ~\cite{murphy2012machine}.

For binary classification, this simplifies to:

\begin{equation}
    H(p,q) = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]
\end{equation}

where $y$ is the true label (0 or 1) and $\hat{y}$ is the predicted probability of the positive class ~\cite{bishop2006pattern}.

In the context of language modeling, where we're predicting the next token in a sequence, the cross-entropy loss is calculated over the entire vocabulary at each step:

\begin{equation}
    L = -\sum_{t=1}^T \sum_{v=1}^V y_{t,v} \log(\hat{y}_{t,v})
\end{equation}

where $T$ is the sequence length, $V$ is the vocabulary size, $y_{t,v}$ is 1 if token $v$ is the true next token at position $t$ and 0 otherwise, and $\hat{y}_{t,v}$ is the model's predicted probability for token $v$ at position $t$ ~\cite{jurafsky2009speech}.

Minimizing this loss encourages the model to assign higher probabilities to the correct next tokens in the training data, thus learning the underlying patterns of the language ~\cite{bengio2003neural}.

Given a neural network architecture $\mathcal{A}$, it induces a parameter space $\Theta \subset \mathbb{R}^d$, where $d$ is the total number of parameters in the network. Each point $\theta \in \Theta$ represents a specific configuration of the network's parameters ~\cite{lecun2015deep}.

Let $\mathcal{L}: \Theta \rightarrow \mathbb{R}$ be the loss function that measures the performance of the model on the given task. The goal of training the neural network can then be formally expressed as an optimization problem:

\begin{equation}
    \theta^* = \arg\min_{\theta \in \Theta} \mathcal{L}(\theta)
\end{equation}

where $\theta^*$ represents the optimal set of parameters that minimizes the loss function over the entire parameter space $\Theta$. This optimization problem encapsulates the essence of training a neural network: finding the configuration of parameters that yields the best performance according to the chosen loss function ~\cite{rumelhart1986learning}.


\subsection{Optimising Parameters}

\subsubsection{Gradient Descent}

Gradient descent is a fundamental optimization algorithm used in training neural networks. It iteratively adjusts the model parameters in the direction of steepest descent of the loss function. The basic update rule for gradient descent is:

\begin{equation}
    \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t)
\end{equation}

where $\theta_t$ are the parameters at iteration $t$, $\eta$ is the learning rate, and $\nabla \mathcal{L}(\theta_t)$ is the gradient of the loss function with respect to the parameters.

Stochastic Gradient Descent (SGD) is a variant that computes the gradient using only a small subset (mini-batch) of the training data in each iteration, making it more computationally efficient for large datasets.

\subsubsection{Automatic Differentiation}

Automatic Differentiation (AutoDiff) allows for efficient and accurate computation of gradients, which are essential for optimization algorithms like gradient descent ~\cite{baydin2018automatic}.

Mathematically, for a composite function $f(g(x))$, the chain rule states:

\begin{equation}
    \frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}
\end{equation}

AutoDiff systems leverage this principle to compute gradients through arbitrarily complex computational graphs ~\cite{griewank2008evaluating}. Modern deep learning frameworks like PyTorch and TensorFlow have built-in AutoDiff capabilities, which greatly simplify the implementation of new models and training procedures ~\cite{paszke2019pytorch, abadi2016tensorflow}. These systems maintain a computational graph of operations and use it to automatically compute gradients, enabling efficient training of large-scale neural networks, allows researchers to focus on designing network architectures and loss functions, while the AutoDiff system handles the intricate details of gradient computation ~\cite{baydin2018automatic}.

Several advanced optimization algorithms have been developed to improve upon basic SGD. Momentum adds a fraction of the previous update to the current one, helping to accelerate convergence and reduce oscillations ~\cite{sutskever2013importance}:
\begin{equation}
    v_{t+1} = \gamma v_t + \eta \nabla \mathcal{L}(\theta_t)
\end{equation}
\begin{equation}
    \theta_{t+1} = \theta_t - v_{t+1}
\end{equation}
where $\gamma$ is the momentum coefficient.

RMSprop adapts the learning rate for each parameter based on the historical gradient magnitudes ~\cite{tieleman2012lecture}:
\begin{equation}
    s_{t+1} = \beta s_t + (1-\beta)(\nabla \mathcal{L}(\theta_t))^2
\end{equation}
\begin{equation}
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_{t+1} + \epsilon}} \nabla \mathcal{L}(\theta_t)
\end{equation}
where $\beta$ is the decay rate and $\epsilon$ is a small constant for numerical stability.

The most widely adopted optimizer in practice, Adam, combines ideas from momentum and RMSprop, maintaining both a running average of gradients and their squared values ~\cite{kingma2014adam}:
\begin{equation}
    m_{t+1} = \beta_1 m_t + (1-\beta_1)\nabla \mathcal{L}(\theta_t)
\end{equation}
\begin{equation}
    v_{t+1} = \beta_2 v_t + (1-\beta_2)(\nabla \mathcal{L}(\theta_t))^2
\end{equation}
\begin{equation}
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_{t+1}} + \epsilon} \hat{m}_{t+1}
\end{equation}
where $\hat{m}_{t+1}$ and $\hat{v}_{t+1}$ are bias-corrected estimates of $m_{t+1}$ and $v_{t+1}$ respectively.

These advanced optimizers often lead to faster convergence and better generalization in practice ~\cite{ruder2016overview}.

\subsubsection{Initialisation}

Proper initialization of neural network parameters is crucial for effective training. Two popular initialization methods have been developed to address this need: Xavier Initialization and He Initialization.

Xavier Initialization, proposed by Glorot and Bengio, is designed to maintain the variance of activations and gradients across layers. For a layer with $n_{in}$ input units and $n_{out}$ output units, weights are initialized from a uniform distribution:

\begin{equation}
    W \sim U\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)
\end{equation}

This initialization works well for layers with linear activations or tanh activations, helping to prevent the vanishing or exploding gradient problem in deep networks.

He Initialization, proposed by He et al., is particularly suited for layers with ReLU activations. In this method, weights are initialized from a normal distribution:

\begin{equation}
    W \sim N\left(0, \sqrt{\frac{2}{n_{in}}}\right)
\end{equation}

This approach helps maintain the variance of the outputs, which is especially important for deep networks with ReLU activations, as it prevents vanishing or exploding gradients.

Both Xavier and He initialization methods aim to set initial parameters that neither shrink nor amplify the input signal excessively. By doing so, they facilitate smoother optimization and faster convergence during the training process, ultimately leading to more effective and efficient neural network models \cite{glorot2010understanding, he2015delving}.

\subsubsection{Normalisation}

In addition to these initialization techniques, normalization methods such as Batch Normalization and Layer Normalization are also employed to address similar issues during training.

Batch Normalization, introduced by Ioffe and Szegedy \cite{ioffe2015batch}, normalizes the inputs of each layer for each mini-batch:
\begin{equation}
    \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
\end{equation}
where $\mu_B$ and $\sigma_B^2$ are the mean and variance of the mini-batch, and $\epsilon$ is a small constant for numerical stability. Batch Normalization helps reduce internal covariate shift, allows higher learning rates, and acts as a regularizer.

Layer Normalization, proposed by Ba et al. \cite{ba2016layer}, normalizes the inputs across the features:
\begin{equation}
    \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
\end{equation}
where $\mu$ and $\sigma^2$ are the mean and variance computed over all features for each data point. Layer Normalization is particularly useful for recurrent neural networks and does not depend on batch size.

Both Batch Normalization and Layer Normalization help stabilize the distribution of layer inputs throughout training, mitigating the vanishing/exploding gradient problem and enabling more efficient training of deep neural networks \cite{santurkar2018does}. These techniques complement initialization methods by maintaining appropriate scaling of activations and gradients throughout the network during the entire training process.

\subsection{Regularisation}

Regularisation techniques are employed to prevent overfitting and improve the generalization of neural networks \cite{goodfellow2016deep}. Overfitting occurs when a model learns the training data too well, including its noise and peculiarities, leading to poor performance on unseen data. This happens because the model becomes too complex and starts to memorize the training examples rather than learning general patterns. Regularisation methods aim to strike a balance between fitting the training data and maintaining simplicity to ensure good performance on new, unseen data \cite{bishop2006pattern}. Some common regularisation methods include:

Dropout randomly "drops out" a proportion of neurons during training, forcing the network to learn more robust features \cite{srivastava2014dropout}. During inference, all neurons are used but their outputs are scaled:

\begin{equation}
    y = f(Wx) \odot m, \quad m_i \sim \text{Bernoulli}(p)
\end{equation}

where $p$ is the probability of keeping a neuron active.

Early Stopping monitors the validation error during training and stops when it starts to increase, preventing the model from overfitting to the training data \cite{prechelt1998early}.

Weight Decay adds a penalty term to the loss function based on the L2 norm of the weights \cite{krogh1992simple}:

\begin{equation}
    \mathcal{L}_{new}(\theta) = \mathcal{L}(\theta) + \lambda \|\theta\|_2^2
\end{equation}

where $\lambda$ is the weight decay coefficient. This encourages smaller weights and simpler models.

Data Augmentation artificially increases the size of the training set by applying transformations to existing data points, helping the model learn invariances and improving generalization \cite{shorten2019survey}.

These regularisation techniques, often used in combination, help in training more robust models that perform well on unseen data \cite{kukavcka2017regularization}.

\section{Historical Language Representations}

Natural Language Processing (NLP) has evolved significantly over the years, with various techniques developed to represent and analyze textual data. This section explores the historical approaches to language representation that laid the foundation for modern NLP methods. We will discuss three key techniques that were widely used before the advent of deep learning and neural network-based models: N-grams, Word Document Frequency, and Topic Modelling. These methods played crucial roles in tasks such as text classification, information retrieval, and document analysis, and their principles continue to influence contemporary NLP approaches \cite{manning1999foundations}.

\subsection{N-grams}
N-grams are contiguous sequences of n items from a given sample of text or speech. In the context of natural language processing, these items are typically words or characters. N-grams have been a fundamental technique in language modeling and various NLP tasks for several decades \cite{jurafsky2000speech}.

The concept of n-grams was introduced in the early days of computational linguistics. One of the seminal papers that popularized the use of n-grams in language modeling is "A Mathematical Theory of Communication" by Claude Shannon (1948), which laid the foundation for information theory and introduced the idea of using probabilistic models for language \cite{shannon1948mathematical}.

N-grams achieved state-of-the-art performance in many NLP tasks throughout the 1990s and 2000s. For instance, the paper "An Empirical Study of Smoothing Techniques for Language Modeling" by Chen and Goodman (1996) demonstrated the effectiveness of various n-gram models for language modeling tasks \cite{chen1996empirical}. Another influential work, "A Bit of Progress in Language Modeling" by Goodman (2001), showed how n-gram models could be extended and improved for better performance \cite{goodman2001bit}.

Google Translate, one of the most widely used machine translation systems, initially relied heavily on n-gram models. Specifically, it used a 5-gram language model, as described in the paper "Large Language Models in Machine Translation" by Brants et al. (2007). This approach allowed Google Translate to capture local context and produce more fluent translations compared to earlier rule-based systems \cite{brants2007large}.

However, despite their historical success, n-grams have limitations that make them less suitable for modern language models. One significant limitation is their restricted context, as n-grams can only capture local context within a fixed window size. This constraint makes it challenging to model long-range dependencies in language, which are crucial for understanding complex sentences and maintaining coherence across paragraphs \cite{bengio2003neural}. Another issue is sparsity; as n increases, the number of possible n-grams grows exponentially, leading to data sparsity issues. This makes it difficult to estimate probabilities for rare or unseen n-grams accurately \cite{katz1987estimation}. N-gram models also lack semantic understanding, treating words as discrete symbols without considering their meaning or relationships. This limitation hinders their ability to capture semantic similarities between words or phrases \cite{mikolov2013distributed}. Additionally, traditional n-gram models operate on a fixed vocabulary, making it difficult to handle out-of-vocabulary words or adapt to new domains \cite{jelinek1980interpolated}. Lastly, the memory requirements for n-gram models can be substantial, as storing probabilities for all possible n-grams can be memory-intensive, especially for large values of n \cite{heafield2011kenlm}.

\begin{tikzpicture}[
    box/.style={draw, rectangle, minimum width=2cm, minimum height=1cm},
    arrow/.style={->, >=stealth, thick},
    node distance=0.5cm
]

% The sentence
\node[anchor=west] (sentence) at (0,0) {Ich \underline{rufe} meinen Freund morgen \underline{an}.};

% N-gram boxes
\node[box, below=1cm of sentence.west, anchor=north west] (box1) {Ich rufe};
\node[box, right=0.2cm of box1] (box2) {rufe meinen};
\node[box, right=0.2cm of box2] (box3) {meinen Freund};
\node[box, right=0.2cm of box3] (box4) {Freund morgen};
\node[box, right=0.2cm of box4] (box5) {morgen an};

% Arrows
\foreach \i in {1,...,4}
    \draw[arrow] (box\i) -- (box\the\numexpr\i+1);

% Explanation
\node[text width=12cm, below=1cm of box3, align=center] (explanation) {
    N-gram model (n=2) fails to capture the relationship between ``rufe'' and ``an''\\
    (separable verb ``anrufen'' = to call)
};

% Highlighting the issue
\draw[red, thick, dashed] ($(sentence.north west)+(0.2cm,1)$) -- ++(1.6cm,0.3cm) -- ++(6.2cm,0) -- ++(-0.5cm,-0.3cm);

\end{tikzpicture}

These limitations have led to the development of more advanced techniques, such as neural network-based language models and transformer architectures, which can capture longer-range dependencies, understand semantic relationships, and generate more coherent and contextually appropriate text.

\subsection{Document Term Matrices}
Document Term Matrices (DTMs) are a fundamental representation in natural language processing and information retrieval. They provide a structured way to represent a collection of documents as a matrix, where each row corresponds to a document and each column represents a unique term in the corpus.

In a basic DTM, each cell contains the frequency of a term in a particular document. 
However, this simple frequency count can be refined using more sophisticated weighting schemes, such as Term Frequency-Inverse Document Frequency (TF-IDF).

\subsubsection{TF-IDF Weighting}
TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents. It is calculated as the product of two components:

\begin{equation}
\text{TF-IDF}(t,d,D) = \text{TF}(t,d) \cdot \text{IDF}(t,D)
\end{equation}

Where:
\begin{equation}
\text{TF}(t,d) = \frac{\text{frequency of term t in document d}}{\text{total number of terms in document d}}
\end{equation}

\begin{equation}
\text{IDF}(t,D) = \log\left(\frac{\text{total number of documents in corpus D}}{\text{number of documents containing term t}}\right)
\end{equation}

TF-IDF weighting helps to reduce the impact of common words that appear frequently across many documents while emphasizing terms that are rare and specific to certain kinds of documents.

\subsubsection{Cosine Similarity}
Cosine similarity is a metric used to measure the similarity between two non-zero vectors in a multi-dimensional space. In the context of DTMs, it is often used to compare documents or queries. The cosine similarity between two documents is calculated as the cosine of the angle between their vector representations in the term space.

Mathematically, for two document vectors A and B, the cosine similarity is given by:

\begin{equation}
\text{cosine similarity} = \frac{A \cdot B}{\|A\| \|B\|}
\end{equation}

Where $A \cdot B$ is the dot product of the vectors, and $\|A\|$ and $\|B\|$ are their Euclidean norms.

A notable example of their application in natural language processing is the work by Burgess et al. in their paper "Legislative Influence Detectors: Identifying Influential Actors in the Legislative Process" \cite{burgess2016legislative}. In this study, the authors used DTMs and cosine similarity to analyze the influence of various actors on legislative outcomes.
The researchers constructed DTMs from legislative texts and stakeholder submissions, then used cosine similarity to measure the textual similarity between final legislation and the various inputs. This allowed them to quantify the influence of different stakeholders on the legislative process, providing valuable insights into policy-making dynamics.

While Document Term Matrices (DTMs) have been widely used in natural language processing, they have significant limitations. Bengio et al. \cite{bengio2003neural} highlighted that traditional n-gram models and DTM-based approaches suffer from the curse of dimensionality and fail to capture semantic similarities between words.
The curse of dimensionality refers to the exponential increase in possible word sequences as sequence length grows, leading to data sparsity and generalization difficulties. For example, with a 10,000-word vocabulary, there are 10,000 possible one-word sequences, but $10^{20}$ possible five-word sequences.
As vocabulary size grows, the number of dimensions increases, potentially leading to overfitting on smaller datasets and processing difficulties with large corpora. Moreover, DTMs fail to capture semantic relationships between words, treating each word as a distinct dimension regardless of meaning. 
This means semantically similar words like "car" and "automobile" are represented as entirely different dimensions.These limitations have motivated the development of word embeddings, which offer a more efficient and semantically rich representation of language. 
Word embeddings are dense vector representations of words in a continuous vector space, typically of much lower dimensionality than DTMs (usually 50-300 dimensions). 
Each word is represented by a fixed-length vector of real numbers encoding semantic and syntactic information, effectively addressing the curse of dimensionality and reducing computational complexity and storage requirements compared to sparse DTMs.

\subsection{Topic Modelling}
Topic modelling is a statistical method for discovering abstract topics that occur in a collection of documents. The most popular and widely used technique is Latent Dirichlet Allocation.

\subsubsection{Latent Dirichlet Allocation (LDA)}
Latent Dirichlet Allocation, introduced by Blei, Ng, and Jordan in 2003 \cite{blei2003latent}, is a generative probabilistic model for collections of discrete data such as text corpora. 
LDA models documents are mixtures of topics, and topics probability distribution over words.

The mathematical formula of LDA can be expressed as follows:

\begin{equation}
p(\mathbf{w}, \mathbf{z}, \boldsymbol{\theta}, \boldsymbol{\phi} | \alpha, \beta) = 
\prod_{d=1}^M p(\theta_d | \alpha) 
\left( \prod_{n=1}^{N_d} p(z_{dn} | \theta_d) p(w_{dn} | z_{dn}, \boldsymbol{\phi}) \right)
\prod_{k=1}^K p(\phi_k | \beta)
\end{equation}

Where:
\begin{itemize}
    \item $\mathbf{w}$ represents the observed words
    \item $\mathbf{z}$ represents the topic assignments
    \item $\boldsymbol{\theta}$ represents the document-topic distributions
    \item $\boldsymbol{\phi}$ represents the topic-word distributions
    \item $\alpha$ and $\beta$ are hyperparameters of the Dirichlet priors
    \item $M$ is the number of documents
    \item $N_d$ is the number of words in document $d$
    \item $K$ is the number of topics
\end{itemize}

Griffiths and Steyvers \cite{griffiths2004finding} applied LDA to analyze the abstracts of papers published in Proceedings of the National Academy of Sciences (PNAS), 
extracting 300 topics from 28,154 abstracts published 1991 to 2001. The model captured the rise and fall of scientific topics over time, reflecting changing research trends.
Topic modelling techniques like LDA has been widely adopted in social sciences, providing researchers with powerful tools to explore and analyze large text collections, 
but superior alternatives based on prompting large language models have emerged ~\cite{WangPrakashPromptTopic}. LDA itself is unsuitable for language modelling  in its
ability for language modelling due to its bag-of-word assumption which hides semantic information and fixed topic assumption.  

\section{Embeddings}
Word embeddings have become a fundamental component of modern natural language processing (NLP) systems, addressing key limitations of traditional representations like Document Term Matrices (DTMs). 
Embeddings are required for several reasons:

Word embeddings offer several key advantages over traditional representations like DTMs. They provide dense, low-dimensional vector representations of words, significantly reducing 
computational complexity and memory requirements for processing large-scale text data. These embeddings capture semantic and syntactic relationships between words in a continuous 
vector space, positioning similar words closer together and enabling models to understand and utilize these relationships. By representing words in a continuous space, embeddings 
facilitate better generalization to unseen words or rare word combinations, addressing the sparsity problem inherent in discrete representations. Additionally, pre-trained word 
embeddings can be used as input features for various NLP tasks, enabling transfer learning and improving performance on tasks with limited training data.

Dense representations offer several key advantages. They enable faster computations in neural networks by leveraging optimized matrix operations, enhancing computational efficiency. 
These representations can compress rich semantic information into a compact form, allowing models to work with more manageable input sizes. The continuous nature of dense representations 
facilitates smoother optimization and better generalization in machine learning models. Additionally, dense embeddings support mathematical operations on words. The canonical example of this 
the example that the vector representations of "king", "man", "woman", "queen" satisfied the following equation approximately (e.g., "king" - "man" + "woman" ≈ "queen"). 
These properties make dense representations particularly valuable in modern NLP systems.

% prehistory  The approach proposed here is also related to previous proposals of character-based text compression using neural networks to predict
% the probability of the next character (Schmidhuber, 1996). The idea of using a neural network for
% language modeling has also been independently proposed by Xu and Rudnicky (2000), although
% experiments are with networks without hidden units and a single input word, which limit the model
% to essentially capturing unigram and bigram statistics

\subsection{A Neural Probabilistic Language Model}
Yoshua Bengio introduced the concept of word embeddings in 2003 \cite{bengio2003neural}, proposing a novel approach that simultaneously learned two crucial components: (1) a distributed feature vector for each word, with a dimensionality of just 30 compared to a vocabulary size of 17,000, and (2) the probability function of word sequences. 
This neural probabilistic language model aimed to learn a low-dimensional representation of words while also capturing the statistical structure of sentences. 
The model consisted of a linear projection layer to obtain the low-dimensional vector followed by a non-linear hidden layer and an output layer to compute the logits.
Bengio noted at the time that training such a model with a million parameters was prohibitively expensive computationally, a topic we will return to. 
In addition, Bengio noted that the embeddings did not capture polysemous words well, and that the model could be improved by using a-priori information, for example from WordNet.
These, also, are prescient in hindsight. The model improved perplexity by 20\% over trigram models. 

\subsection{word2vec}
In 2013, Tomas Mikolov and colleagues at Google introduced word2vec, a highly efficient method for learning high-quality word embeddings \cite{mikolov2013efficient}. Word2vec significantly improved upon previous methods by dramatically reducing computational complexity while maintaining or improving the quality of the resulting word vectors.

Word2vec proposed two model architectures: Continuous Bag-of-Words (CBOW) and Skip-gram. Both models use a shallow neural network to learn word embeddings, but they differ in their objectives. 
The CBOW model predicts a target word given its context words. The context is typically a symmetric window of words around the target.
On the other hand, the skip-gram model does the opposite, predicting the context words given a target word.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{cbow_diagram.png}
        \caption{Continuous Bag-of-Words (CBOW) Model}
        \label{fig:cbow}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{skipgram_diagram.png}
        \caption{Skip-gram Model}
        \label{fig:skipgram}
    \end{subfigure}
    \caption{Word2Vec Model Architectures}
    \label{fig:word2vec}
\end{figure}

The vector embeddings are learned through the process of training these models. As the model tries to predict words based on their context (or vice versa), it adjusts the word vectors to maximize the probability of correct predictions. This process causes words that appear in similar contexts to have similar vector representations.

One of the most striking features of word2vec embeddings is their ability to capture semantic relationships between words. This is often demonstrated through vector arithmetic. For example, the famous analogy "king - man + woman ≈ queen" shows that the learned embeddings encode meaningful semantic and syntactic relationships.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{word2vec_analogy.png}
    \caption{Word2Vec Analogy: king - man + woman ≈ queen}
    \label{fig:word2vec_analogy}
\end{figure}

Word2vec's efficiency comes from several training optimizations, including negative sampling and hierarchical softmax, which allow it to be trained on large corpora quickly. This efficiency, combined with the quality of the resulting embeddings, led to word2vec becoming a cornerstone of many NLP applications and paved the way for further advancements in word embedding techniques.

\subsection{GloVe}
It's worth noting that the idea of using co-occurrence to create word embeddings had been raised in the past in the field of Information Retrieval, where feature vectors for words are learned on the basis of their probability of co-occurring in the same documents (Latent Semantic Indexing, see Deerwester et al., 1990).
Global Vectors for Word Representation (GloVe), introduced by Pennington et al. \cite{pennington2014glove}, introduced a popular word embedding method that combines the advantages of two major model families: global matrix factorization and local context window methods, based on the insight that ratios of word-word co-occurrence probabilities have the potential to encode meaning components. 
The model learns word vectors by minimizing the following objective function:

\begin{equation}
J = \sum_{i,j=1}^V f(X_{ij})(\mathbf{w}_i^T\tilde{\mathbf{w}}_j + b_i + \tilde{b}_j - \log X_{ij})^2
\end{equation}

Where:
\begin{itemize}
    \item $V$ is the size of the vocabulary
    \item $X_{ij}$ is the number of times word $j$ occurs in the context of word $i$
    \item $\mathbf{w}_i$ and $\tilde{\mathbf{w}}_j$ are word vectors
    \item $b_i$ and $\tilde{b}_j$ are scalar biases
    \item $f(X_{ij})$ is a weighting function
\end{itemize}

When it was introduced, GloVe showed excellent performance on various NLP tasks, comparable to that of word2vec. 

\subsection{ELMo}
ELMo (Embeddings from Language Models), introduced by Peters et al. in 2018 \cite{peters2018deep}, marked a significant advancement in word embedding techniques. Unlike its predecessors, ELMo produces dynamic, contextualized word embeddings that capture complex characteristics of word use, including syntax and semantics, and how these uses vary across linguistic contexts.

The key innovation of ELMo lies in its use of a deep, bidirectional LSTM model trained on a large text corpus. This model consists of a character-level convolutional neural network (CNN) that captures subword information and a two-layer bidirectional LSTM that processes the output of the CNN
ELMo generates word embeddings by taking a weighted sum of the internal states of this LSTM. Importantly, these weights are learned as part of the downstream task, allowing the model to emphasize different aspects of the word representations depending on the specific application.

ELMo advanced the state of the art by allowing to represent different meanings of the same word depending on context and capturing syntactic information in the embeddings, improving performance on tasks like part-of-speech tagging. It was improved by BERT which relied on the creation of the transformer architecture, which emerged from Sequence Learning. We'll take a brief detour to discuss the history of sequence learning. 

\section{Sequence Learning}
Sequence learning is a fundamental task in natural language processing and machine learning, involving the prediction or generation of sequences of data. In the context of NLP, this often means working with sequences of words or tokens. Two significant advancements in this field are the seq2seq (sequence-to-sequence) model and its application to neural machine translation.

\subsection{Sequence-to-Sequence (seq2seq) Models}
Sequence-to-sequence models, introduced by Sutskever et al. \cite{sutskever2014sequence}, are a class of models designed to transform an input sequence into an output sequence. 
These models typically consist of an encoder, which processes the input sequence and compresses it into a fixed-length context vector, and a decoder which takes the context vector and generates the output sequence.
The encoder and decoder were implemented using Long Short-Term Memory (LSTM) networks, due to their ability to handle long-range dependencies in sequences.

\subsection{Neural Machine Translation}
Neural Machine Translation (NMT) is one of the most prominent applications of seq2seq models.
Bahdanau et al. \cite{bahdanau2014neural} introduced the attention mechanism, allowing their neural machine translation model to 'attend' to different parts of the input sequence when generating each word of the output sequence, which significantly improved translation quality.
The attention mechanism can be formalized as follows:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

Where:
\begin{itemize}
    \item $Q$ is the query matrix
    \item $K$ is the key matrix
    \item $V$ is the value matrix
    \item $d_k$ is the dimension of the keys
\end{itemize}

This mechanism allows the model to weigh the importance of different parts of the input when producing each part of the output. The dot product of the query with all the keys measures how much attention to pay to each value when producing an output.

Another vital technique from NMT adopted in current language models is Byte Pair Encoding (BPE). This was originally a method for data compression \cite{PhilipGage1994}, but was adapted to neural machine translation to help address the problem of representing rare or out of vocab words\cite{sennrich2015neural}. 
BPE iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. In the context of NLP, it operates on characters or subword units rather than bytes. BPE helps address the issue of out-of-vocabulary words by breaking words into subword units. 
This approach significantly improves the model's ability to generalize across different languages and domains, making it particularly useful in machine translation and other multilingual NLP tasks.

\section{Transformers and Beyond}
The field of NLP underwent a revolutionary change with the introduction of the Transformer architecture, as presented in the seminal paper "Attention Is All You Need" by Vaswani et al. \cite{vaswani2017attention}. This paper introduced a novel architecture that relies entirely on attention mechanisms, dispensing with recurrence and convolutions entirely.

The Transformer architecture, Similar to seq2seq models, has an encoder-decoder structure. It introduced Multi-Head Attention, which allows the model to simultaneously focus on different parts of the input for various purposes, extending the attention mechanism to jointly attend to information from different representation subspaces at different positions. 

In contrast to vanilla Attention, the Multi-Head Attention mechanism can be formalized as follows:

\begin{equation}
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\end{equation}

where each head is computed as:

\begin{equation}
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{equation}

Here:
\begin{itemize}
    \item $Q$, $K$, and $V$ are the query, key, and value matrices
    \item $W_i^Q$, $W_i^K$, $W_i^V$ are learned parameter matrices for each head
    \item $W^O$ is the output projection matrix
    \item $h$ is the number of attention heads
\end{itemize}

This mechanism allows the model to jointly attend to information from different representation subspaces at different positions, enhancing the model's ability to capture various aspects of the input data.

The transformer architecture relies on positional embeddings, most commonly RoPE, as the model lacks recurrence or convolution, providing information about the sequence order. Each layer in both the encoder and decoder contains a fully connected feed forward network. Additionally, layer normalisatoin are employed to facilitate more effective training of deeper networks.

One of the most significant advantages of the Transformer architecture is its parallelizability. Unlike recurrent models (like LSTMs) that process sequences step by step, Transformers can process entire sequences in parallel. This is possible because the self-attention mechanism computes relationships between all words in a sequence simultaneously. This parallelization allows for much faster training on modern hardware like GPUs and TPUs, enabling the development of larger and more powerful models.

\subsection{BERT}
Building upon the Transformer architecture, Bidirectional Encoder Representations from Transformers (BERT), introduced by Devlin et al. \cite{devlin2018bert}, marked another significant milestone in NLP. 
BERT revolutionized the field by applying bidirectional training of Transformer to language modeling, leading to contextual representations that capture meaning from both left and right contexts.

BERT's bidirectional context considers the entire context of a word by looking at the words that come before and after it. 
This approach allows BERT to develop a deeper sense of language context and flow. 
BERT also introduced novel pretraining tasks: the Masked Language Model (MLM), where it randomly masks 15\% of the tokens in the input and predicts the masked tokens, as well as Next Sentence Prediction (NSP),
where it learns to predict if a given sentence naturally follows another. These tasks force the model to maintain a contextual representation of every input token and understand relationships between sentences.
The model uses the Transformer encoder architecture, typically with 12 or 24 Transformer layers, 768 or 1024 hidden units, and 12 or 16 attention heads. It employs learned positional embeddings with a sequence length of 512 tokens.

After pretraining, BERT can be fine-tuned with just one additional output layer for a wide variety of NLP tasks, including sentiment analysis, natural language inference, named entity recognition, question answering, and text classification. 
This fine-tuning process allows BERT to adapt its pretrained knowledge to specific downstream tasks with minimal task-specific parameters, leading to state-of-the-art results on numerous NLP benchmarks.

BERT's success demonstrated the power of unsupervised pretraining and transfer learning in NLP, paving the way for a new paradigm in language understanding models. 
It significantly outperformed previous methods on eleven NLP tasks, including achieving human-level performance on SQuAD v1.1. 
However, BERT has limitations, such as a fixed-length context window and potential pretraining-finetuning mismatch. 
Despite these, BERT's impact on NLP has been profound, inspiring numerous follow-up works and improvements that have further advanced the field of natural language understanding and generation.
Models like RoBERTa (larger training data and no NSP) \cite{liu2019roberta}, ALBERT (significant parameter reduction) \cite{lan2019albert}, and DistilBERT (knowledge distillation) \cite{sanh2019distilbert} improved upon BERT, however they won't be discussed in the interest of time. 



\subsubsection{GPT-2}
GPT-2, introduced by Radford et al. in 2019 \cite{radford2019language}, was a groundbreaking language model that demonstrated the potential of large-scale unsupervised pre-training. With 1.5 billion parameters, GPT-2 was trained on a diverse dataset of 8 million web pages, aiming to predict the next word in a given sequence of text. This model showed remarkable zero-shot task transfer abilities, performing well on various NLP tasks without specific fine-tuning.

Unlike typical Transformer models that include both encoder and decoder components, GPT-2 exclusively uses the decoder part. This design choice focuses on generating text based on the learned context, making it highly effective for tasks like text completion and generation. This architecture is particularly well-suited for language modeling tasks.

The success of GPT-2 highlighted a crucial insight: increasing model size and training data volume could lead to significant improvements in performance and generalization. 
This observation set the stage for the development of its successor, GPT-3.

\subsubsection{Scaling Laws}
The progression from GPT-2 to GPT-3 was guided by insights into the scaling behavior of language models. Kaplan et al. \cite{kaplan2020scaling} conducted a comprehensive study on the scaling laws for language models, published concurrently with the development of GPT-3. Their research revealed that performance scales as a power-law with model size, dataset size, and computational budget. Furthermore, they found that models are significantly more sample-efficient the larger they are, requiring fewer training steps to achieve the same performance as smaller models. In addition, they observed a trade-off between compute-optimal and data-optimal scaling, with different optimal strategies depending on the available resources.

A prominent scaling law called "Chinchilla scaling" emerged from subsequent research by Hoffmann et al. \cite{hoffmann2022training}. This law states that for optimal performance, model size (N) and dataset size (D) should be scaled in approximately equal proportions as the compute budget increases. This is expressed mathematically as:

\begin{equation}
C = C_0ND
\end{equation}

\begin{equation}
L = AN^\alpha + BD^\beta + L_0
\end{equation}

Where C is compute, L is loss, and the other terms are constants.

Chinchilla scaling provides a more nuanced understanding of the relationship between model size, dataset size, and computational resources. It suggests that simply increasing model size without a corresponding increase in dataset size may not yield optimal results, challenging the "bigger is always better" assumption.

These scaling laws have provided a theoretical foundation for the development of GPT-3 and continue to influence the design of subsequent language models. They offer valuable guidance for researchers and developers in making decisions about model architecture, dataset size, and allocation of computational resources, ultimately driving the field towards more efficient and powerful AI systems.

\subsubsection{GPT-3}
Building upon the foundations laid by GPT-2, Brown et al. introduced GPT-3 in 2020 \cite{brown2020language}. 
GPT-3 represented a massive increase in scale, boasting 175 billion parameters—more than 100 times larger than its predecessor. 
This substantial increase in model size was accompanied by a correspondingly large training dataset, comprising approximately 500 billion tokens.
GPT-3's architecture remained similar to GPT-2, utilizing the decoder-only transformer architecture. 
However, its unprecedented scale led to remarkable improvements in performance across a wide range of tasks. Notably, GPT-3 demonstrated strong few-shot and zero-shot learning capabilities, often matching or surpassing the performance of models specifically fine-tuned for particular tasks.

\subsection{Emergent Behaviour in Language Models}
GPT-3 revealed intriguing emergent behaviors, capabilities that were not explicitly programmed or anticipated. These behaviors often become apparent only as models are scaled up in size and complexity.

One key aspect of emergent behavior is captured by the aforementioned scaling laws described by Kaplan et al. \cite{kaplan2020scaling}. These laws suggest that model performance improves predictably with increases in model size, dataset size, and computational resources. This observation aligns with the "bitter lesson" articulated by Rich Sutton \cite{sutton2019bitter}, which posits that methods leveraging increased amounts of computation tend to outperform those with intricate new architectures.

A particularly interesting emergent phenomenon is "grokking," first described by Power et al. \cite{power2022grokking}. Grokking refers to a sudden performance improvement after a long period of stagnation during training. This phenomenon suggests that large language models might undergo phase transitions in their learning process, suddenly grasping underlying patterns in ways that are not yet fully understood.

These emergent behaviors highlight the potential for continued improvements as we scale up language models, but also underscore the challenges in understanding and controlling these complex systems.

\subsection{Aligning Language Models with Human Intent}
GPT-3 wasn't specifically trained to follow instructions or assist users in a helpful manner. InstructGPT, developed by Ouyang et al. 
\cite{ouyang2022training}, addressed this limitation through a process of fine-tuning.
The process involves several steps:

1. Supervised Fine-Tuning (SFT): The model is fine-tuned on a dataset of human-written demonstrations of desired behavior. The optimization objective for SFT can be expressed as:

   \[\mathcal{L}_{\text{SFT}} = -\mathbb{E}_{(x,y)\sim \mathcal{D}_{\text{SFT}}}[\log p_\theta(y|x)]\]

   where $\mathcal{D}_{\text{SFT}}$ is the dataset of human demonstrations, $x$ is the input, $y$ is the desired output, and $p_\theta$ is the model with parameters $\theta$.

2. Reward Modeling (RM): A reward model is trained to predict human preferences between model outputs.

3. Reinforcement Learning from Human Feedback (RLHF): The SFT model is further optimized using Proximal Policy Optimization (PPO) 
with the reward model as the reward function \cite{schulman2017proximal}. PPO iteratively refines the model's policy while maintaining stability through a 'trust region' approach that keeps updates to within a specified boundary. It uses a clipped objective function to balance improvement with consistency:

   \[\mathcal{L}_{\text{PPO}} = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]\]

   where $r_t(\theta)$ is the probability ratio comparing current and old policies, $\hat{A}_t$ is the estimated advantage, and $\epsilon$ controls the clipping range.

PPO also incorporates a Kullback-Leibler (KL) divergence penalty to further ensure the new policy doesn't deviate too far from the old one. The Kullback-Leibler divergence is an information theoretical measure of the difference between two probability distributions.

This process results in a model more aligned with human intentions, capable of following specific instructions, and generally more helpful and safe in its outputs. InstructGPT demonstrated improved performance on instruction-following tasks while reducing unwanted behaviors.

An alternative to PPO-based RLHF is Direct Preference Optimization (DPO), introduced by Rafailov et al. \cite{rafailov2023direct}. 
DPO simplifies the process by directly optimizing the policy to match human preferences without the need for a separate reward model or reinforcement learning. 
The DPO objective can be expressed as:
\[\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}}[\log \sigma(\beta(\log p_\theta(y_w|x) - \log p_\theta(y_l|x)))]\]
where $y_w$ and $y_l$ are the preferred and non-preferred outputs respectively, $\sigma$ is the sigmoid function, and $\beta$ is a temperature parameter. DPO offers a more stable and computationally efficient approach to aligning language models with human preferences. Although DPO reduces complexity by removing the need for a reward model, it has been shown to have limitations. \cite{dposuperiortoppo_xu}

\subsection{Few-Shot and Zero-Shot Learning}
Large Language Models have an extraordinary ability to perform few-shot and zero-shot learning. These capabilities allow LLMs to adapt to new tasks with minimal or no task-specific training examples \cite{brown2020language}.
Few-shot learning denotes the model's ability to perform a new task after seeing only a small number of examples \cite{wang2020generalizing}. This approach has shown remarkable success in various NLP tasks, including text classification, named entity recognition, and question answering \cite{gao2021making}. 
Zero-shot learning, on the other hand, involves the model's capacity to perform tasks without any task-specific examples \cite{xian2018zero}. 
This capability relies heavily on the model's pre-training and the formulation of the task in natural language. 
Radford et al. \cite{radford2019language} demonstrated zero-shot task transfer with GPT-2 already. For instance, when given a news article and prompted with "TL;DR:" the model was able to generate a summary of the article without any specific training for summarization tasks. This showcases the model's ability to understand and perform a new task based solely on the context provided in the prompt, illustrating the potential for language models to generalize to unseen tasks. This capability enables models to perform tasks without explicit fine-tuning, making them highly versatile and adaptable to new scenarios \cite{min2022rethinking}.

Few and zero shot learning are especially promising in low resource domains, where task-specific labeled data is scarce \cite{hedderich2021survey}, for example low-resource language translation \cite{garcia2020multilingual} and domain adaptation \cite{gururangan2020don}. These capabilities of LLMs greatly diminish the cost required to create 'intelligent' applications, by allowing developers to craft prompts which activate latent spaces of an LLM's distribution to perform well at a novel task, without the requirement of having a propreitary dataset to fine tune on.

\subsubsection{Prompting}

Prompt engineering techniques play an important role in optimizing few-shot and zero-shot capabilities \cite{liu2021pre}. 
Prompt engineerings consists of carefully crafting input prompts to elicit the desired behavior from the model. 
Strategies such as chain-of-thought prompting \cite{wei2022chain}, self-consistency \cite{wang2022self}, and ReAct \cite{yao2023react} have shown significant improvements in model performance across various tasks. Chain-of-thought prompting encourages the model to break down complex problems into intermediate steps, mimicking human reasoning. Self-consistency involves generating multiple independent reasoning paths and aggregating their results to improve reliability. ReAct (Reasoning and Acting) interleaves reasoning traces and task-specific actions, allowing models to plan, reason, and interact with external environments more effectively.


\subsection{Multimodal Models}
Large Language Models (LLMs) have expanded from text-only processing to incorporate multiple modalities. This integration allows for more comprehensive understanding and generation of content across different media types. A significant advancement in multimodal processing is the development of CLIP (Contrastive Language-Image Pre-training) by Radford et al. \cite{radford2021cliplearning}. CLIP connects visual and textual information through a joint embedding space. The model is trained on a diverse dataset of image-text pairs from the internet, learning to associate images with their textual descriptions. It consists an image encoder, typically a vision transformer and a text encoder, usually a transformer-based model. These encoders are trained to maximize the cosine similarity between the embeddings of matching image-text pairs while minimizing it for non-matching pairs. This training approach enables CLIP to perform zero-shot classification and open-ended image recognition tasks without specific fine-tuning.

\section{Language Model Applications}

\subsection{Structured Output}
The ability to generate structured output is a crucial application of language models, extending them beyond mere text generation to more specialized and format-specific tasks. Recent research emphasizes the importance of implementing structured output constraints in LLMs to enhance their integration into developer workflows and improve user experience. These constraints ensure outputs adhere to specific formats and semantic guidelines, facilitating more reliable and predictable results when chaining multiple LLM calls together. This approach streamlines development processes and makes LLM-powered applications more user-friendly and dependable.

LangChain, a common LLM chaining framework, implements structured output generation using Pydantic models to guide LLMs through prompts. This approach involves defining a Pydantic model, incorporating the schema into prompts, and parsing the LLM's response. While flexible, this method has limitations, with increased costs due to longer prompts and lack of guaranteed conformity to the specified structure.

Logit processing is an alternative for guaranteeing structured generation in language models \cite{chaudhari2023logit}. A key component of this approach is selective multiplication, which computes logits only for tokens allowed by the structure, thereby guaranteeing conformity. This process can be formalized as $W_uX[:,\ell]$, where $W_u \in \mathbb{R}^{N_V \times d_e}$ represents the unembedding matrix, $X$ denotes the encoded token sequence, and $\ell$ is the sequence length.

This method has demonstrated the potential to make structured generation faster than its unstructured counterpart, with particular efficacy when the number of allowed next tokens is small. The implementation of this technique occurs through model-level integration, ensuring seamless incorporation into existing language model architectures \cite{chaudhari2023logit}. This approach effectively addresses a fundamental computational inefficiency in traditional language model inference, where logits are computed for the entire vocabulary, even when many tokens are impossible given the structure \cite{willard2023efficient}. Empirical studies have shown consistent speed improvements across both simple and complex schemas. For instance, in the case of a simple JSON schema, structured generation utilizing selective multiplication consistently outperformed traditional methods in terms of speed. Similarly, for more complex schemas, significant time savings were observed, although the frequency of optimization application was reduced \cite{chaudhari2023logit}.

\subsection{RAG (Retrieval-Augmented Generation)}
Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of large language models (LLMs) with external knowledge retrieval systems to enhance the quality and reliability of generated responses \cite{lewis2020retrieval}. This approach addresses several key challenges in LLM applications, particularly in scenarios requiring up-to-date or specialized information.
One of the primary benefits of RAG is its ability to enhance LLM responses with external knowledge. By incorporating a retrieval component, RAG systems can access and utilize vast amounts of information that may not be present in the LLM's training data \cite{lewis2020retrieval}. This is particularly valuable for tasks requiring domain-specific knowledge or current events information.
A significant advantage of RAG is its capacity to reduce hallucinations in LLM outputs. Hallucinations, where models generate plausible but factually incorrect information, are a common issue with large language models \cite{maynez2020faithfulness}. By grounding responses in retrieved information, RAG systems can significantly mitigate this problem, leading to more reliable and trustworthy outputs \cite{lewis2020retrieval}.
The integration of external knowledge retrieval also contributes to improving the factual accuracy of LLM-generated content. This is crucial in applications where precision and reliability are paramount, such as in medical, legal, or scientific domains \cite{karpukhin2020dense}. RAG systems can provide citations or references to support generated information, enhancing transparency and verifiability.
RAG has found particularly successful applications in question-answering systems. By combining the natural language understanding capabilities of LLMs with the ability to retrieve relevant information from large databases, RAG-based systems can provide more accurate, contextually appropriate, and up-to-date answers to user queries \cite{guu2020retrieval}. This has implications for improving search engines, virtual assistants, and customer support systems.
Recent advancements in RAG include the development of more sophisticated retrieval mechanisms, such as dense passage retrieval \cite{karpukhin2020dense}, and the integration of RAG with other techniques like few-shot learning and prompt engineering \cite{liu2021pre}. These developments continue to expand the capabilities and applications of RAG in various fields, from open-domain question answering to specialized information retrieval tasks.


\subsubsection{Efficient Vector Search}
Retrieval-Augmented Generation (RAG) systems heavily rely on efficient vector search mechanisms, particularly approximate nearest neighbor (ANN) search algorithms \cite{johnson2019billion}. These algorithms are crucial for quickly identifying the most relevant information to augment language model outputs. The efficiency of vector search directly impacts the performance and usability of RAG systems, especially in real-time applications where response time is critical \cite{lewis2020retrieval}. Exhaustive search methods quickly become impractical for large-scale datasets due to their linear time complexity. This necessitates the use of approximate methods that can deliver sufficiently accurate results at a fraction of the computational cost \cite{andoni2018approximate}.
Various indexing strategies have been developed to optimize vector search for large-scale datasets. These strategies often involve preprocessing the data to create efficient data structures that facilitate faster search operations. Common approaches include tree-based methods (e.g., KD-trees, Ball trees) and graph-based methods \cite{malkov2018efficient}. Two innovations used by Moon AI are detailed belowed.

\subsubsubsection{Matryoshka Embeddings}
Matryoshka Embeddings, introduced by Hoffer et al. \cite{hoffer2018deep}, represent an innovative approach to improving the efficiency of vector search operations. This technique introduces an auxiliary objective during the training of embedding models, resulting in a hierarchical representation of semantic information. With Matryoshka Embeddings, the truncated versions of the embeddings are trained to maintain the similarity relationship through the auxiliary objective. This property allows for a unique search strategy: starting with a low-dimensional (and thus computationally cheaper) version of the embedding, and progressively increasing the dimensionality only for the most promising candidates. This approach can significantly speed up retrieval through successive embedding model calls, effectively reducing the computational cost of similarity search without substantially sacrificing accuracy \cite{hoffer2018deep}.The hierarchical nature of Matryoshka Embeddings also provides flexibility in balancing computational resources and search a ccuracy. Depending on the specific requirements of a search, more or fewer dimensions of the embedding can be used, dynamically adjusting the trade-off between speed and precision \cite{hoffer2018deep}.

\subsubsubsection{HNSW (Hierarchical Navigable Small World)}
The Hierarchical Navigable Small World (HNSW) algorithm \cite{malkov2018efficient} is a state of the art method for efficient ANN search, which is commonly used in vector search applications. HNSW constructs a multi-layer graph structure of "navigable small worlds," enabling logarithmic complexity in search operations. The algorithm begins at the top layer with the fewest nodes and refines the search in lower, more detailed layers, quickly narrowing down the search space.

In vector search applications, HNSW excels at finding similar vectors in high-dimensional spaces, making it ideal for tasks like semantic similarity search in RAG systems \cite{johnson2019billion}. Its ability to balance search quality and memory usage through parameter tuning (e.g., number of layers, connections per node) allows for optimization in various scenarios, from memory-constrained environments to high-performance systems.

\subsection{Inference: Expanding Context Lengths and Decreasing Costs}
The field of language model inference has seen significant advancements in expanding context lengths and reducing computational costs. These developments are crucial for improving the performance and applicability of large language models (LLMs) across various domains.
One notable innovation is the Longformer, introduced by Beltagy et al. \cite{beltagy2020longformer}, which presents an efficient attention mechanism for processing long sequences. The Longformer combines local windowed attention with task-motivated global attention, allowing it to handle documents with tens of thousands of tokens while maintaining computational efficiency.
Sparse attention patterns have emerged as a key strategy for managing long sequences. Child et al. \cite{child2019generating} proposed the Sparse Transformer, which uses fixed sparse attention patterns to reduce the quadratic complexity of self-attention to O(n√n). This approach enables the processing of much longer sequences than traditional transformer models.
These advancements have significant applications in document-level tasks. For instance, Zaheer et al. \cite{zaheer2020big} demonstrated the effectiveness of sparse attention models in tasks such as long-document classification and question-answering, where understanding the entire document context is crucial.
Recent innovations have further improved the efficiency of LLM inference. Flash Attention, introduced by Dao et al. \cite{dao2022flashattention}, is a memory-efficient attention algorithm that significantly reduces the memory footprint and computational complexity of attention mechanisms. By leveraging hardware-aware techniques and recomputation strategies, Flash Attention enables faster training and inference of large language models, particularly for long sequences.
Another promising development is the introduction of Linear Projection Units (LPUs) by Hua et al. \cite{hua2022transformer}. LPUs offer an alternative to the traditional feed-forward networks in transformer architectures, providing comparable performance with reduced computational costs. By replacing non-linear activations with learned linear projections, LPUs can accelerate inference while maintaining model quality.
Hardware acceleration continues to play a vital role in improving LLM inference. Techniques such as quantization, pruning, and specialized hardware designs are being explored to further optimize the performance and energy efficiency of LLM deployments.
These advancements collectively contribute to expanding the practical applications of LLMs, enabling their use in scenarios requiring processing of longer documents and real-time responses, while simultaneously working towards more cost-effective and efficient inference.

\subsection{Agents}
The concept of AI agents, particularly those based on large language models (LLMs), has gained significant attention recently due to their potential for autonomous decision-making and complex task solving. This subsection explores key aspects of LLM-based agents and their implications.
One notable contribution to this field is the "Simulacra" paper by Li et al. \cite{li2023simulacra}, which investigates emergent behavior in LLM-based agents. The authors demonstrate that these agents can exhibit complex, seemingly intelligent behaviors that were not explicitly programmed.
Multi-agent systems have shown promise in addressing complex tasks that are beyond the scope of single-agent approaches. Dafoe et al. \cite{dafoe2020open} discuss the potential of multi-agent AI systems in solving intricate, economically valuable problems across various domains.
These systems leverage the collective intelligence of multiple LLM-based agents, often leading to more robust and creative solutions.The integration of reinforcement learning (RL) with LLMs has opened new avenues for creating more adaptive and goal-oriented agents. Luketina et al. \cite{luketina2019survey} provide a comprehensive survey of the intersection between deep learning, reinforcement learning, and natural language processing, highlighting how RL can enhance LLM-based agents' ability to learn from interaction and optimize for specific objectives.

\chapter{Venture Capital}
\section{Overview}
Venture capital (VC) plays a crucial role in fostering innovation and economic growth by providing funding to high-potential startups and early-stage companies \cite{gompers2001venture}. This form of private equity investment is particularly relevant to the dynamism of an economy, as it fuels entrepreneurship and technological advancements \cite{kortum2000assessing_contribution_venture_capital}.

The VC ecosystem primarily consists of two key players: Limited Partners (LPs) and General Partners (GPs). LPs, typically institutional investors or high-net-worth individuals, provide the capital, while GPs manage the fund and make investment decisions \cite{metrick2010venture}. This structure allows for a pooling of resources and expertise to support promising ventures.

VC funds operate on a lifecycle model, usually spanning 7-10 years \cite{gompers2004venture}. During this period, GPs actively manage the fund, making investments and nurturing portfolio companies. Management fees, typically around 2\% of committed capital annually, compensate GPs for their operational expenses \cite{gompers2016structure}.

Investments in startups are often categorized into stages: Seed, Series A, B, C, and later stages. Each stage represents a different phase of a company's growth and involves varying levels of capital and risk \cite{hellmann2002venture}. Seed funding supports early concept development, while later stages focus on scaling and expansion.

The venture capital investment lifecycle encompasses several critical stages, each contributing to the strategic deployment of capital in high-potential startups \cite{gompers2004venture}. Initially, deal sourcing involves identifying promising investment opportunities through various channels, including industry networks and proprietary databases \cite{sorenson2001syndication}. Subsequently, initial screening filters potential investments based on predetermined criteria aligned with the VC firm's investment thesis \cite{petty1994harvesting}. The due diligence phase then entails a comprehensive evaluation of selected startups, scrutinizing their business models, market potential, team capabilities, and financial projections \cite{fried2003venture}. Following a positive assessment, term sheet negotiation and deal closing formalize the investment structure \cite{kaplan2003financial, cumming2006contracts}. Post-investment, VCs typically provide ongoing support to portfolio companies, leveraging their expertise and networks to enhance value \cite{sapienza1996venture}. The lifecycle culminates in an exit event, such as an IPO or acquisition, realizing returns on the investment \cite{cumming2008preplanned}. This systematic approach underscores the rigorous and strategic nature of venture capital investing, from initial opportunity identification to final value realization.

\section{VC fund structure}


The structure of a typical venture capital (VC) fund is designed to align the interests of investors (Limited Partners or LPs) and fund managers (General Partners or GPs). This structure can be formalized using several key parameters, which we define as follows:

Let f ∈ (0, ∞) represent the Fund Size, which specifically refers to paid-in-capital, the net amount that LPs commit to a given fund inclusive of all fees. The Lifespan of the fund in years is denoted by l ∈ (0, ∞). The annual Management Fee, charged to the GPs for each year of the fund's lifespan, is represented by p ∈ [0, 1]. Often VC funds require a GP Commit, a percentage of the fund f that the GPs must personally commit. The Carry Fee, c ∈ [0, 1], is calculated as a percentage of LP distributions that GPs are entitled to after returning the principal. Finally, we define the Expected Multiple, m ∈ [0, ∞), as the distributions to paid-in capital (DPI) that a GP expects to return.

The investment cycle of a VC fund typically consists of two main periods: the Investment Period, usually spanning the first 3-5 years of the fund's life, during which GPs make new investments; and the Harvest Period, comprising the remaining years, focused on managing and exiting investments.

The distribution of returns is governed by a Distribution Waterfall, which often includes a "hurdle rate" that must be met before GPs can claim carried interest. Additionally, a Clawback Provision ensures that GPs do not receive more than their agreed-upon share of profits over the fund's lifetime.

The most prevalent structure is a "2 and 20" model (2\% management fee and 20\% carried interest) which is designed to incentivize GPs to maximize returns while providing a stable operational base. 

The utility function for a GP in this model can be expressed as the sum of the carry incentive and the management fee incentive:

\begin{equation}
U_{GP} = U_{GPc} + U_{GPm}
\end{equation}

\begin{equation}
U_{GPm} = \sum_{L=0}^l fp
\end{equation}

\begin{equation}
U_{GPc} = ((f - U_{GP}^p)m - (f - U_{GP}^p))c
\end{equation}

Under this model, assuming a GP simply wants to maximise their utility, the "highest leverage action is to increase the size of the fund or increase the management fee, rather than improve the value of the underlying equity" \cite{Jafri_distributed_vc}. MoonAI's goal of building autonomous sourcing and due diligence in VC can help to solve this dislocation of incentives. Reducing the capital needed to run a VC fund through intelligent automations puts negative pressure on management fees. Therfore, it helps align incentives with founders and LPs by forcing GPs to be incentivised by their carry and thus the underlying equity. 

Before considering how MoonAI is structured, it is important to consider that the carry is generated by power law startups. 

\section{Power Law}

The concept of power law returns in venture capital has profound implications for portfolio construction strategies. This phenomenon, characterized by a small number of investments generating the majority of returns, fundamentally shapes how venture capitalists approach fund management and investment decisions \cite{peter2015power}.

In venture capital, returns typically follow a power law distribution, which can be mathematically expressed as:

\begin{equation}
P(x) \propto x^{-\alpha}
\end{equation}

Where $P(x)$ is the probability of an investment returning $x$ times the initial investment, and $\alpha$ is the power law exponent, typically between 2 and 3 for venture capital returns \cite{clauset2009power}. This distribution implies that the probability of extremely high returns, while low, is significantly higher than what would be expected in a normal distribution.

A crucial aspect of VC portfolio construction is the concept of "returning the fund." VCs typically look for each investment to have the potential to return the entire fund, meaning that a single successful exit could provide returns equal to or greater than the total size of the fund \cite{feld2016venture}. This approach is directly informed by the power law distribution of returns.

For example, in a \$100 million fund promising a 3x return with 20 investments, VCs are not looking for each company to return \$5 million. Instead, they're hoping that at least one company of the twenty investments might return \$300 million or more, effectively "returning the fund" on its own. This strategy acknowledges that many investments will fail or provide modest returns, but seeks to capture the rare outliers that can deliver outsized returns and drive overall fund performance.

This "return the fund" mentality leads VCs to focus on companies with massive potential markets and scalable business models. It also explains why VCs are often willing to accept high valuations for promising startups - the potential upside of a fund-returning investment can justify taking on significant risk and paying a premium price.


\section{Data-Driven Investing}
The venture capital industry has shifted from a bespoke asset class at its inception in the 1940s to a commoditised institution on the whole. This heightened competition has led to a shift towards more data-driven strategies as VCs seek to gain an edge in the market by leveraging analytics in the sourcing and due diligence of investment cases, as well as management of portfolio startups. In the following subsections, we will provide an overview of some data-driven methods currently employed in the venture capital industry. These approaches demonstrate how the field is evolving to incorporate more quantitative and technological solutions to address the challenges of startup evaluation and investment decision-making.

\subsubsection{Investment Specific Language Models}
Recent advancements in language models have led to the development of investment-specific models designed to enhance financial analysis and decision-making. Yang et al. introduced InvestLM, a model that fine-tunes LLaMA-65B using a curated corpus of financial documents \cite{yang2023investlm}. The model employs Low Rank Adaptation (LoRA) for efficient parameter tuning and Linear Rope Scaling to handle large input lengths. Evaluation by domain experts and financial NLP benchmarks demonstrated InvestLM's strong capabilities in comprehending financial text and responding to investment-related queries.

The development of InvestLM contributes to the growing field of financial domain language models, which aim to process extensive financial texts and improve investment decision-making. However, it is noted that many of these models, including BloombergGPT and InvestLM, primarily focus on "high finance" applications \cite{yang2023investlm}. The study also provides support for the superficial alignment hypothesis, suggesting that knowledge acquired during pre-training and fine-tuning primarily aligns output with specific formats.

Other notable contributions in this field include FinBERT, a financial language model based on BERT architecture; FinMA, which fine-tunes language models on financial NLP benchmarks with instruction-formatted questions; and FinGPT, which focuses on news and tweet sentiment analysis, albeit with limited generalizability to other tasks \cite{araci2019finbert, liu2023finma, yang2023fingpt}. These models represent the ongoing efforts to adapt large language models to specific financial applications, although challenges remain in achieving broad applicability across various financial tasks.

\subsubsection{Network-Based Approach to Predicting Success}

Venture capital ecosystems can be modelled as complex graphs, and research has focused on using graph structure to infer success likelihood. Two notable approaches in this area are bipartite link prediction and graph neural networks.

One study applied link prediction techniques to bipartite graphs for identifying potential investment opportunities in venture capital networks \cite{predictVCbipartite}. It modeled the investment landscape as a bipartite graph, with investors and companies as nodes, and investments as edges. The study compared three link prediction models: Random Link Prediction (baseline), Preferential Attachment (PA), and Segmented Preferential Attachment (SPA). Results showed that PA and SPA significantly outperformed random prediction, with PA achieving better overall F1 scores and SPA demonstrating higher maximum precision.

In contrast, a more recent study by Lyu et al. \cite{lyu2023graph} employed graph neural networks for predicting startup success. This approach utilized a comprehensive dataset from PitchBook and proposed an incremental graph representation learning method. The key components of this method were multifaceted. Firstly, it involved the construction of a bipartite graph network that represented the intricate relationships between individuals and startups. Secondly, the method incorporated Graph Self-Attention (GST) for node-level representation learning, enabling the capture of complex interactions within the network. Thirdly, it implemented an incremental updating mechanism for representations as the network evolved over time, ensuring the model's adaptability to dynamic ecosystems. Fourthly, the approach employed supervised fine-tuning using link prediction and node classification tasks, enhancing the model's predictive capabilities. Lastly, it utilized sequential modeling of node representations over time using Long Short-Term Memory (LSTM) networks, allowing for the capture of temporal dependencies in startup trajectories.

The graph neural network approach outperformed all baselines, including human investors and traditional machine learning methods, achieving up to 1.94x better performance than human investors. It showed particularly strong results for startups in IT, healthcare, and B2C industries.

Comparing the two approaches, the graph neural network method appears to be more sophisticated and effective. While the bipartite link prediction study focused solely on predicting potential investments, the graph neural network approach aimed to predict startup success (defined as IPO or acquisition within 5 years). The latter also incorporated more complex features and temporal dynamics, leading to superior performance.

Both studies highlight the potential of graph-based machine learning techniques in venture capital, but the graph neural network approach demonstrates a more comprehensive and powerful method for leveraging network structures in startup success prediction.

\subsubsection{Success Prediction}

This section examines a study on Web-based Startup Success Prediction by Sharchilev et al., which presents an innovative approach to predicting startup success. The research enriches structured data from the startup ecosystem with information from business-oriented social networking services and general web presence. The study employs a robust machine learning pipeline, utilizing multiple base models and gradient boosting techniques.

The authors address the challenge of defining startup success, acknowledging the limitations of various metrics such as revenue, mergers and acquisitions, and funding rounds. They also discuss alternative modeling approaches in the literature, including portfolio optimization and link prediction, noting that these methods introduce additional uncertainty compared to direct discriminative success prediction.

The study's problem statement focuses on predicting whether a startup that has received seed or angel funding will secure a Series A or larger funding round within the next year. The dataset encompasses general company information, previous investment details, team information, and web mentions.

Methodologically, the researchers developed a robust and diversified prediction pipeline called WBSSP, which combines several machine learning models. The pipeline utilizes CatBoost, a gradient boosting decision tree modification, with features derived from dense neural network outputs, logistic regression, and a neural network fed by a sparse neural network.

The results demonstrate state-of-the-art performance, with a significant finding being the importance of a company's web presence. While individual web mentions may not be significant, their aggregation provides a representative picture of a company's perception by its target audience, substantially improving prediction quality compared to using only structured data from the startup ecosystem.

Title: Choosing the Right Friends: Social Network Analysis for Entrepreneurial Success
Key Points:

The study analyzes the relationship between social networks and success for entrepreneurs and academics, focusing on Swiss startups and networking organizations.
Proximity to key influencers (e.g., directors, advisors) in social networks like LinkedIn, Facebook, and email correlates with business success.
Participation in entrepreneurship programs (VentureKick and venture leaders) is mutually reinforcing and associated with higher success rates.
Entrepreneurs with profiles on professional networking sites (e.g., XING) tend to be more successful.
ETH Zurich alumni are significantly more successful and better connected in entrepreneurial networks compared to graduates from other Swiss universities.
The study found positive correlations between network centrality (both degree and betweenness) and success, particularly within the ETH alumni network.
Geographic proximity and face-to-face relationships still play a crucial role in business success, despite increasing virtual connections.
The research suggests a positive feedback loop: better-connected individuals become more successful, and successful individuals become better connected.
Recommendations for entrepreneurs include:

Studying at selective universities like ETH Zurich
Building strong links with fellow students and alumni
Participating in startup coaching programs
Engaging with business networking organizations
Utilizing social media for business networking


The study emphasizes the importance of pre-existing social capital acquired through university and entrepreneurship programs for later business success.

The research provides insights into the value of social networks and networking organizations for startup entrepreneurs, highlighting the role of both online and offline connections in fostering business success.

\subsection{Hypothesis Extraction}

Many VC funds operate on the basis of specific theses or industry focus. Investment professionals within these funds aim to identify promising companies that align with their predetermined sectors or theses. Consequently, the automated inference of sector compatibility and thesis alignment for potential investment targets is a critical factor in the success of thematic VC strategies.


A team at EQT, a well known player in the data driven VC industry, developed a novel methodology for extracting industry hypotheses and investment theses from company descriptions utilizing generative language models ~\cite{lele_cao_infer_industry_sector}. The authors propose a scalable and adaptive system that employs prompt engineering and fine-tuning of medium-sized generative language models. It utilized a hierarchical sector framework for multi-granular representation and it also implemented a hybrid approach combining both prompt tuning and model fine-tuning.

The proposed system demonstrated superior performance compared to classification baselines, achieving an accuracy exceeding 80\%. Furthermore, it exhibits generalization capabilities to previously unseen sectors absent from the training data. The system also demonstrates robustness in handling dynamic sector frameworks and evolving annotations.

This research first showed the potential of generative language models in extracting structured industry hypotheses from unstructured company data at scale, with the possibility of expanding it to extract other tags that are relevant to a VC. 

\chapter{Moon AI}
\subsection{Architecture}

\chapter{Conclusion}


% This displays the bibliography for all cited external documents. All references have to be defined in the file references.bib and can then be cited from within this document.
\bibliographystyle{IEEEtran}
\bibliography{references}

% This creates an appendix chapter, comment if not needed.
\appendix
\chapter{First Appendix Chapter Title}

\end{document}